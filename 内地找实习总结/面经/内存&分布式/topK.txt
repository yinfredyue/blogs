注：以下讨论的是top-k most frequent，如果只是单纯数值上的top-k, 原理
完全相同，只是不要用hashmap统计频率。

四种情况：大数据/小数据，单机/分布式
其中小数据-分布式没有意义，不讨论。

1. 小数据-单机
https://juejin.im/post/5c4f0146518825469414e8a2
a. 简单排序. O(nlogn)
b. 类似quickSelect的partition的想法。O(n), in-place.
   注意：quickSelect不仅仅可以用于找到“第k大”的数字，还可以用于
   寻找“前k大”的数字！
c. 维持一个容量为k的最大堆。O(nlogk), priorityQueue.

https://blog.csdn.net/juzihongle1/article/details/70212243
2. 大数据-单机
1G大小的文件，每行是一个词，词不超过16字节，内存限制是1M。返回频数最高的100个词。

由于大数据和内存限制，小数据-单机中的方法都不适用：
a, b都需要将所有数据读入内存，不适用。
c不需要将整个文件读入内存，os也支持每次读取文件的一部分，但是在读取过
程中必须存储每个词（不仅仅是topK词）对应的频率，依然不适用。

处理大文件的常见思路：将大文件分割成小文件。可以使用Hash(x) % N的方式
将大文件中的词分到N个小文件中，然后操作每一个小文件。

算法：
(1) 顺序读取文件，每次读取一部分。对每个词x, 用Hash(x) % 2000将大文件
分割成2000个小文件中，这样每个file的平均大小是512KB。如果分割中发现
依然超过1M，则继续用hash分割。复杂度：O(N)。需要注意的是在这个过程中，
同一个词只会在一个file中存在！（很重要）
极端情况是一个file不够存一个词，比如1G的文件存的都是相同的一个词。我们
可以用一个单独的Hashmap来存储这种非常频繁的词。在步骤(1)中我们可以发现
一个file都是同一个词。把这个词加到hashmap中，频率设为当前file的行数，
之后遇到该词时不分到小文件中，只是增加Hashmap中的counter

(2) 对于每一个小file，使用hashmap统计每个词的词频。统计完成后，用一个
大小为100的最大堆选择这个file中频率前100的单词。将这100个词及频率存入
另一个文件中。复杂度：O(N) + O(N*log(100)) = O(N)。

(3) 用大小为100的最小堆，将得到的每一个含有<词-频率>的file遍历一遍，
找出所有词中的top100。如果hashmap中有词，也遍历hashmap。复杂度: O(N)。
注意：这个方法正确性的前提是我们用hash来分割file，所以保证相同的词都在
同一个file中！如果没有这个保证，这种方法是错误的。

变形：海量日志数据，每行是一个IP，找出最高频的IP。

3. 大数据-分布式
海量数据在100台电脑中，统计这批数据最频繁出现的topK。

虽然数据已经是分布式的了，但是如果求各自的topK再合并的话是错误的！所以
依然需要使用hash将数据中重复的数据整合到同一个机器上，然后可以求各自的
topK然后合并。

(1) 使用hash将每个文件中的数据重新分配到不同机器上。
(2) 使用单机topK统计该机器上的每个file的topK，然后统计该机器上的topK
(3) 使用容量为k的最大堆，合并所有机器上的topK.


其他变式：
# 一万行的文本，每行是一个词，统计最频繁的topK。
思路：一万行不算多。hashmap + maxHeap.

# 十亿行的文本，每行一个词，统计最频繁topK。
思路：与大数据-单机思路相同。分割 + hashmap + maxHeap。

# 100万个数据中找出最大的100个。
思路：假如可以全部读入内存(100万 = 2 ^ 20，每个数字8byte，总共8MB），
使用小数据-单机的方法。
